
# ğŸ“± Sentiment Analysis Platform

A fullstack platform to scrape app reviews from Google Play and Apple App Store, perform sentiment analysis using fine-tuned BERT models, compare applications, track model performance, and monitor infrastructure â€” scalable and production-ready.

---

allows users to search for apps, analyze sentiment (positive, negative, neutral), compare reviews between platforms, and share results via unique links.


## Usage

1. **Search Apps**: Enter an app name and select Google Play Store or Apple App Store.
2. **Sentiment Analysis**: Reviews are analyzed and classified as positive, negative, or neutral.
3. **Comparison**: Compare reviews between platforms and share analysis results via unique links.
4. **Notifications**: Stay updated when reviews are fetched or new data is available.

---

### Scraping
- Multithreaded scrapers ensure backend performance is non-blocking.
- Use rotating proxies to prevent IP bans.

### Sentiment Analysis
- The platform uses BERT for sentiment classification, which can be fine-tuned further.

### Database
- Optimized schema with indexes for app IDs and deduplication.
- Scales efficiently with high query volume.



## ğŸ“š Project Structure

```plaintext
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ lifespan.py 
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ scraper.py
â”‚   â”‚   â”‚   â”œâ”€â”€ sentiment.py
â”‚   â”‚   â”‚   â”œâ”€â”€ comparison.py
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â”œâ”€â”€ notification.py
â”‚   â”‚       â”œâ”€â”€ threading.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ sentiment_bert/                 # Fine-tuned BERT models
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”œâ”€â”€ app_schema.py
â”‚   â”‚   â”œâ”€â”€ review_schema.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ base_service.py    
â”‚   â”‚   â”œâ”€â”€ review_service.py    
â”‚   â”‚   â”œâ”€â”€ google_review_scraper.py
â”‚   â”‚   â”œâ”€â”€ apple_review_scraper.py
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ test_review_service.py
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ connection.py                    # Connect to external PostgreSQL
â”‚   â”‚   â”œâ”€â”€ queries.py
â”‚   â”‚   â”œâ”€â”€ migration_runner.py              # Manual class-based migration runner
â”‚   â”‚   â””â”€â”€ migrations/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ base_migration.py
â”‚   â”‚       â”œâ”€â”€ 001_create_apps_table.py
â”‚   â”‚       â”œâ”€â”€ 002_create_reviews_table.py
â”‚   â”‚       â”œâ”€â”€ 003_create_search_history_table.py
â”‚   â”œâ”€â”€ airflow/
â”‚   â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”‚   â”œâ”€â”€ scrape_reviews_dag.py
â”‚   â”‚   â”‚   â”œâ”€â”€ retrain_model_dag.py
â”‚   â”‚   â”œâ”€â”€ plugins/
â”‚   â”‚   â”‚   â””â”€â”€ telegram_alert.py
â”‚   â”œâ”€â”€ mlflow_server/
â”‚   â”‚   â””â”€â”€ config/
â”‚   â”‚       â””â”€â”€ mlflow.cfg
â”‚   â”œâ”€â”€ monitoring/
â”‚   â”‚   â”œâ”€â”€ grafana/
â”‚   â”‚   â”‚   â”œâ”€â”€ grafana.ini
â”‚   â”‚   â”‚   â””â”€â”€ dashboards/
â”‚   â”‚   â”‚       â”œâ”€â”€ system_metrics.json
â”‚   â”‚   â”‚       â”œâ”€â”€ scraping_metrics.json
â”‚   â”‚   â”œâ”€â”€ prometheus/
â”‚   â”‚   â”‚   â””â”€â”€ prometheus.yml
â”‚   â”œâ”€â”€ docker/
â”‚   â”‚   â”œâ”€â”€ Dockerfile.backend
â”‚   â”‚   â”œâ”€â”€ Dockerfile.airflow
â”‚   â”‚   â”œâ”€â”€ Dockerfile.mlflow
â”‚   â”‚   â”œâ”€â”€ Dockerfile.prometheus
â”‚   â”‚   â”œâ”€â”€ Dockerfile.grafana
â”‚   â”œâ”€â”€ cron.py                              
â”‚   â”œâ”€â”€ main.py                              # FastAPI application entry
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ SearchBar.jsx
â”‚   â”‚   â”œâ”€â”€ AppComparison.jsx
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ index.js
â”‚   â”‚   â”œâ”€â”€ compare/[id].js
â”‚   â”œâ”€â”€ redux/
â”‚   â”‚   â”œâ”€â”€ store.js
â”‚   â”‚   â”œâ”€â”€ slices/
â”‚   â”‚   â”‚   â”œâ”€â”€ appSlice.js
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ api.js
â”‚   â”œâ”€â”€ Dockerfile.frontend
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ initial_reviews_google.csv
â”‚   â”‚   â”œâ”€â”€ initial_reviews_apple.csv
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ labeled_reviews.csv
â”‚   â”œâ”€â”€ README.md                            
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ sentiment_experiment.ipynb            
â”‚   â”œâ”€â”€ scraper_experiment.ipynb              
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ airflow/
â”‚   â”‚   â””â”€â”€ airflow.cfg
â”‚   â”œâ”€â”€ grafana/
â”‚   â”‚   â”œâ”€â”€ grafana.ini
â”‚   â”‚   â””â”€â”€ dashboards/
â”‚   â”‚       â”œâ”€â”€ system_metrics.json
â”‚   â”‚       â”œâ”€â”€ scraping_metrics.json
â”‚   â”œâ”€â”€ mlflow/
â”‚   â”‚   â””â”€â”€ mlflow.cfg
â”‚   â”œâ”€â”€ deployment/
â”‚   â”‚   â”œâ”€â”€ docker-compose.prod.yml          # Production Swarm Compose
â”‚   â”‚   â”œâ”€â”€ traefik.yml                      # Traefik reverse proxy config
â”œâ”€â”€ .env.development                         # Development environment variables
â”œâ”€â”€ .env.production                          # Production environment variables
â”œâ”€â”€ docker-compose.yml                       # Local dev docker-compose
â”œâ”€â”€ Makefile                                 # Automation commands
â”œâ”€â”€ README.md                                # Project Documentation


- `backend/`: FastAPI app, scrapers, BERT fine-tuning, business logic
- `frontend/`: Next.js (with Redux Toolkit) web frontend
- `datasets/`: Initial datasets (raw, processed)
- `notebooks/`: Experimental Jupyter notebooks
- `infrastructure/`: Configurations and deployments (Airflow, Grafana, MLflow, Traefik)
- `.env.*`: Environment-specific variables
- `docker-compose.yml`: Development stack definition
- `README.md`: Documentation
```

---

## ğŸ›  Technology Stack

| Layer | Technology |
|:------|:-----------|
| Frontend | Next.js, Redux Toolkit |
| Backend | FastAPI, Starlette, Pydantic |
| Scraping | Airflow Orchestration |
| ML Models | BERT fine-tuning, HuggingFace, MLflow |
| Database | External PostgreSQL (cloud-managed or separate server) |
| Monitoring | Prometheus + Grafana |
| Infrastructure | Docker Swarm, Traefik, Spot VMs (GCP) |
| Notifications | Email Alerts + Telegram API |

---

## ğŸ“¦ Environment Management

The system uses **different `.env` files** for development and production:

| Environment | File | Purpose |
|:------------|:-----|:--------|
| Development | `.env.development` | Connects to local or test database |
| Production  | `.env.production`  | Connects to cloud-hosted or external database |

**Before running** docker-compose or make commands, set the correct environment:

- For **development**:
  ```bash
  make set_env_dev
  make up
  ```

- For **production**:
  ```bash
  make set_env_prod
  docker stack deploy -c infrastructure/deployment/docker-compose.prod.yml your_stack_name
  ```

> **Note**: `backend` connects to database via `DATABASE_URL` environment variable only.

---

## ğŸ›  Using the Makefile

The Makefile automates all essential tasks:

| Command | Description |
|:--------|:------------|
| `make build` | Build all backend docker images |
| `make start_backend` | Start the FastAPI backend service |
| `make start_monitoring` | Start Grafana and Prometheus |
| `make start_airflow` | Start Airflow scheduler, webserver, worker |
| `make migrate` | Run manual class-based database migrations |
| `make up` | Bring up all services |
| `make down` | Bring down all services |
| `make logs` | View service logs |
| `make clean` | Clean up containers and volumes |
| `make restart` | Restart all services |
| `make build_and_up` | Build images and start services |
| `make set_env_dev` | Copy `.env.development` to `.env` |
| `make set_env_prod` | Copy `.env.production` to `.env` |

---


## ğŸ› ï¸ Development Workflow with Makefile

This project uses a **Makefile** to simplify common tasks.

### Available Commands:

| Command | Description |
|:-------:|:-----------:|
| `make install` | Install Python dependencies into the virtual environment |
| `make migrate` | Run database migrations |
| `make backend-dev` | Start the FastAPI backend server (with Hot Reload) |
| `make docker-build` | Build Docker images |
| `make docker-up` | Start all Docker containers (backend, airflow, mlflow, etc.) |
| `make docker-down` | Stop all Docker containers |
| `make docker-restart` | Restart Docker containers cleanly |
| `make test` | Run backend unit tests |

---



## ğŸ› ï¸ Makefile Commands Reference

### âš™ï¸ Setup & Install

| Command | Description |
|--------|-------------|
| `make install` | Install Python dependencies using `pip` in the virtualenv |
| `make check-env` | Ensure `.env` file exists |
| `make check-venv` | Ensure Python virtualenv exists |
| `make load-env` | Export all variables from `.env` into the shell |

### ğŸš€ Application Startup

| Command | Description |
|--------|-------------|
| `make up` | Smart startup: starts Postgres, waits, initializes Airflow if needed, then starts everything |
| `make backend-dev` | Run FastAPI dev server (`uvicorn`) |
| `make bootstrap-airflow` | Init Airflow DB if needed and restart services |

### ğŸ³ Docker Infrastructure

| Command | Description |
|--------|-------------|
| `make docker-up` | Start all Docker containers |
| `make docker-down` | Stop all containers |
| `make docker-restart` | Restart all containers |
| `make docker-build` | Rebuild all Docker images |
| `make docker-status` | Show Docker container statuses |
| `make docker-logs` | Show container logs |
| `make docker-prune` | Remove unused Docker resources |

### ğŸ” Database

| Command | Description |
|--------|-------------|
| `make migrate` | Run database migrations |
| `make backup-db` | Backup PostgreSQL to timestamped `.sql` |

### ğŸŒ¬ï¸ Airflow

| Command | Description |
|--------|-------------|
| `make airflow-init` | Initialize Airflow DB |
| `make airflow-version` | Show Airflow version |
| `make health-check` | Ping services: FastAPI, Airflow, Postgres |

### âš ï¸ Dev-Only

| Command | Description |
|--------|-------------|
| `make kill-all` | âš ï¸ Wipe all Docker volumes + containers (with prompt) |



### ğŸ§© Setup

```bash
# 1. Clone the repo
git clone https://github.com/your-username/sentiment-analysis-platform.git
cd sentiment-analysis-platform

# 2. Create a virtual environment manually if it doesn't exist
python3 -m venv env_sent

# 3. Activate your virtual environment
source env_sent/bin/activate

# 4. Install all Python dependencies
make install

# 5. Create a .env file (copy from .env.example if available)
cp .env.example .env

# 6. Run database migrations
make migrate

# 7. Start backend server for development
make backend-dev
```


## ğŸ§  Features

- **App Review Scraping** from Google Play Store and Apple App Store
- **Sentiment Analysis** (Positive, Neutral, Negative) using fine-tuned BERT models
- **App Comparison** between platforms
- **Scheduled Retraining** via Airflow DAGs
- **Scraper Orchestration** using Airflow
- **Model Tracking** using MLflow
- **Monitoring and Alerting** with Prometheus, Grafana, Email, Telegram
- **Dockerized Services**, **Swarm Deployment** ready
- **External PostgreSQL Database** for persistence and scaling

---

## ğŸ—„ Datasets

Initial data is stored inside:

```plaintext
datasets/
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ initial_reviews_google.csv
â”‚   â”œâ”€â”€ initial_reviews_apple.csv
â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ labeled_reviews.csv
```

- `raw/`: Raw scraped data (unlabeled)
- `processed/`: Auto-labeled or human-labeled sentiment data for model fine-tuning

---

## ğŸš¨ Important Notes

- **Database**: No internal Postgres container is provided. Use a separate managed PostgreSQL server.
- **Production Readiness**: Use GCP Spot VMs for heavy model training; Hetzner CPU server for inference.
- **Security**: Set strong `.env` secrets for production.
- **Scaling**: Ready for Docker Swarm clustering and Traefik load balancing.

---

## ğŸš€ Quick Start (Local Development)

```bash
# Setup local env
make set_env_dev

# Build images
make build

# Run services
make up

# Access services
- Backend API: http://localhost:8000
- Airflow Web UI: http://localhost:8080
- Grafana Dashboards: http://localhost:3000
- MLflow Tracking: http://localhost:5000
```

---

## ğŸ“œ License

MIT License - Feel free to use, modify, and contribute.

---
